<?xml version='1.0' encoding='iso-8859-1' ?>
<documents><document>
<title>Feature Extraction and Recognition for Human Action Recognition</title>
<publication-date>2014-05-01T00:00:00-07:00</publication-date>
<state>published</state>
<authors>
<author>
<email>jluo9@utk.edu</email>
<institution>University of Tennessee - Knoxville</institution>
<lname>Luo</lname>
<fname>Jiajia</fname>
</author>
</authors>
<keywords>
<keyword>feature extraction</keyword>
<keyword>feature representation</keyword>
<keyword>dictionary learning</keyword>
<keyword>sparse coding</keyword>
</keywords>
<disciplines><discipline>Other Computer Engineering</discipline>
</disciplines><abstract>&lt;p&gt;How to automatically label videos containing human motions is the task of human action recognition. Traditional human action recognition algorithms use the RGB videos as input, and it is a challenging task because of the large intra-class variations of actions, cluttered background, possible camera movement, and illumination variations. Recently, the introduction of cost-effective depth cameras provides a new possibility to address difficult issues. However, it also brings new challenges such as noisy depth maps and time alignment. In this dissertation, effective and computationally efficient feature extraction and recognition algorithms are proposed for human action recognition.&lt;/p&gt;
&lt;p&gt;At the feature extraction step, two novel spatial-temporal feature descriptors are proposed which can be combined with local feature detectors. The first proposed descriptor is the Shape and Motion Local Ternary Pattern (SMltp) descriptor which can dramatically reduced the number of features generated by dense sampling without sacrificing the accuracy. In addition, the Center-Symmetric Motion Local Ternary Pattern (CS-Mltp) descriptor is proposed, which describes the spatial and temporal gradients-like features. Both descriptors (SMltp and CS-Mltp) take advantage of the Local Binary Pattern (LBP) texture operator in terms of tolerance to illumination change, robustness in homogeneous region and computational efficiency.&lt;/p&gt;
&lt;p&gt;For better feature representation, this dissertation presents a new Dictionary Learning (DL) method to learn an overcomplete set of representative vectors (atoms) so that any input feature can be approximated by a linear combination of these atoms with minimum reconstruction error. Instead of simultaneously learning one overcomplete dictionary for all classes, we learn class-specific sub-dictionaries to increase the discrimination. In addition, the group sparsity and the geometry constraint are added to the learning process to further increase the discriminative power, so that features are well reconstructed by atoms from the same class and features from the same class with high similarity will be forced to have similar coefficients.&lt;/p&gt;
&lt;p&gt;To evaluate the proposed algorithms, three applications including single view action recognition, distributed multi-view action recognition, and RGB-D action recognition have been explored. Experimental results on benchmark datasets and comparative analyses with the state-of-the-art methods show the effectiveness and merits of the proposed algorithms.&lt;/p&gt;</abstract>
<coverpage-url>https://trace.tennessee.edu/utk_graddiss/2710</coverpage-url>
<fulltext-url>https://trace.tennessee.edu/cgi/viewcontent.cgi?article=4018&amp;amp;context=utk_graddiss&amp;amp;unstamped=1</fulltext-url>
<label>2710</label>
<document-type>dissertation</document-type>
<type>article</type>
<articleid>4018</articleid>
<submission-date>2014-02-24T22:39:28-08:00</submission-date>
<publication-title>Doctoral Dissertations</publication-title>
<context-key>5202381</context-key>
<submission-path>utk_graddiss/2710</submission-path>
<fields>
<field name="advisor1" type="string">
<value>Hairong Qi</value>
</field>
<field name="advisor2" type="string">
<value>Mongi A. Abidi, Peter K. Liaw, Husheng Li, Qing Cao</value>
</field>
<field name="degree_name" type="string">
<value>Doctor of Philosophy</value>
</field>
<field name="department" type="string">
<value>Computer Engineering</value>
</field>
<field name="embargo_date" type="date">
<value>2011-01-01T00:00:00-08:00</value>
</field>
<field name="instruct" type="string">
<value>1</value>
</field>
<field name="publication_date" type="date">
<value>2014-05-01T00:00:00-07:00</value>
</field>
</fields>
</document>
</documents>