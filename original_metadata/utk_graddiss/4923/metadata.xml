<?xml version='1.0' encoding='iso-8859-1' ?>
<documents><document>
<title>Binary Representation Learning for Large Scale Visual Data</title>
<publication-date>2018-05-12T00:00:00-07:00</publication-date>
<state>published</state>
<authors>
<author>
<email>lliu25@vols.utk.edu</email>
<institution>University of Tennessee</institution>
<lname>Liu</lname>
<fname>Liu</fname>
</author>
</authors>
<keywords>
<keyword>Image Hashing</keyword>
<keyword>Discriminative Learning</keyword>
<keyword>Natural Language Processing</keyword>
<keyword>Adversarial Learning</keyword>
<keyword>Object Detection</keyword>
<keyword>Computer Vision</keyword>
</keywords>
<abstract>&lt;p&gt;The exponentially growing modern media created large amount of multimodal or multidomain visual data, which usually reside in high dimensional space. And it is crucial to provide not only effective but also efficient understanding of the data.In this dissertation, we focus on learning binary representation of visual dataset, whose primary use has been hash code for retrieval purpose. Simultaneously it serves as multifunctional feature that can also be used for various computer vision tasks. Essentially, this is achieved by discriminative learning that preserves the supervision information in the binary representation.By using deep networks such as convolutional neural networks (CNNs) as backbones, and effective binary embedding algorithm that is seamlessly integrated into the learning process, we achieve state-of-the art performance on several settings. First, we study the supervised binary representation learning problem by using label information directly instead of pairwise similarity or triplet loss. By considering images and associated textual information, we study the cross-modal representation learning. CNNs are used in both image and text embedding, and we are able to perform retrieval and prediction across these modalities. Furthermore, by utilizing unlabeled images from a different domain, we propose to use adversarial learning to connect these domains. Finally, we also consider progressive learning for more efficient learning and instance-level representation learning to provide finer granularity understanding. This dissertation demonstrates that binary representation is versatile and powerful under various circumstances with different tasks.&lt;/p&gt;</abstract>
<coverpage-url>https://trace.tennessee.edu/utk_graddiss/4923</coverpage-url>
<fulltext-url>https://trace.tennessee.edu/cgi/viewcontent.cgi?article=6464&amp;amp;context=utk_graddiss&amp;amp;unstamped=1</fulltext-url>
<label>4923</label>
<document-type>dissertation</document-type>
<type>article</type>
<articleid>6464</articleid>
<submission-date>2018-12-03T13:13:10-08:00</submission-date>
<publication-title>Doctoral Dissertations</publication-title>
<context-key>13415051</context-key>
<submission-path>utk_graddiss/4923</submission-path>
<fields>
<field name="advisor1" type="string">
<value>Hairong Qi</value>
</field>
<field name="advisor2" type="string">
<value>Jens Gregor, Husheng Li, Russell L. Zaretzki</value>
</field>
<field name="author1_orcid" type="string">
<value>http://orcid.org/0000-0001-9090-8300</value>
</field>
<field name="degree_name" type="string">
<value>Doctor of Philosophy</value>
</field>
<field name="department" type="string">
<value>Computer Engineering</value>
</field>
<field name="publication_date" type="date">
<value>2018-05-12T00:00:00-07:00</value>
</field>
</fields>
</document>
</documents>