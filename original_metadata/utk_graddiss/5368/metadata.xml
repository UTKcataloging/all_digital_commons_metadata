<?xml version='1.0' encoding='iso-8859-1' ?>
<documents><document>
<title>Cross domain Image Transformation and Generation by Deep Learning</title>
<publication-date>2019-05-01T00:00:00-07:00</publication-date>
<state>published</state>
<authors>
<author>
<institution>University of Tennessee</institution>
<lname>Song</lname>
<fname>Yang</fname>
</author>
</authors>
<keywords>
<keyword>Generative model</keyword>
<keyword>cross-domain</keyword>
<keyword>domain transfer</keyword>
<keyword>deep learning</keyword>
</keywords>
<abstract>&lt;p&gt;Compared with single domain learning, cross-domain learning is more challenging due to the large domain variation. In addition, cross-domain image synthesis is more difficult than other cross learning problems, including, for example, correlation analysis, indexing, and retrieval, because it needs to learn complex function which contains image details for photo-realism. This work investigates cross-domain image synthesis in two common and challenging tasks, i.e., image-to-image and non-image-to-image transfer/synthesis.The image-to-image transfer is investigated in Chapter 2, where we develop a method for transformation between face images and sketch images while preserving the identity. Different from existing works that conduct domain transfer in a one-pass manner, we design a recurrent bidirectional transformation network (r-BTN), which allows bidirectional domain transfer in an integrated framework. More importantly, it could perceptually compose partial inputs from two domains to simultaneously synthesize face and sketch images with consistent identity. Most existing works could well synthesize images from patches that cover at least 70% of the original image. The proposed r-BTN could yield appealing results from patches that cover less than 10% because of the recursive estimation of the missing region in an incremental manner. Extensive experiments have been conducted to demonstrate the superior performance of r-BTN as compared to existing solutions.Chapter 3 targets at image transformation/synthesis from non-image sources, i.e., generating talking face based on the audio input. Existing works either do not consider temporal dependency thus yielding abrupt facial/lip movement or are limited to the generation for a specific person thus lacking generalization capacity. A novel conditional recurrent generation network which incorporates image and audio features in the recurrent unit for temporal dependency is proposed such that smooth transition can be achieved for lip and facial movements. To achieve image- and video-realism, we adopt a pair of spatial-temporal discriminators. Accurate lip synchronization is essential to the success of talking face video generation where we construct a lip-reading discriminator to boost the accuracy of lip synchronization. Extensive experiments demonstrate the superiority of our framework over the state-of-the-arts in terms of visual quality, lip sync accuracy, and smooth transition regarding lip and facial movement.&lt;/p&gt;</abstract>
<coverpage-url>https://trace.tennessee.edu/utk_graddiss/5368</coverpage-url>
<fulltext-url>https://trace.tennessee.edu/cgi/viewcontent.cgi?article=6904&amp;amp;context=utk_graddiss&amp;amp;unstamped=1</fulltext-url>
<label>5368</label>
<document-type>dissertation</document-type>
<type>article</type>
<articleid>6904</articleid>
<submission-date>2020-02-14T08:42:03-08:00</submission-date>
<publication-title>Doctoral Dissertations</publication-title>
<context-key>16545376</context-key>
<submission-path>utk_graddiss/5368</submission-path>
<fields>
<field name="advisor1" type="string">
<value>Hairong Qi Professor</value>
</field>
<field name="advisor2" type="string">
<value>Jens Gregor Professor, Russell Zaretzki Professor, Arvind Ramanathan Dr.</value>
</field>
<field name="degree_name" type="string">
<value>Doctor of Philosophy</value>
</field>
<field name="department" type="string">
<value>Computer Engineering</value>
</field>
<field name="publication_date" type="date">
<value>2019-05-01T00:00:00-07:00</value>
</field>
</fields>
</document>
</documents>