<?xml version='1.0' encoding='iso-8859-1' ?>
<documents><document>
<title>A Resource Efficient Localized Recurrent Neural Network Architecture and Learning Algorithm</title>
<publication-date>2006-12-01T00:00:00-08:00</publication-date>
<state>published</state>
<authors>
<author>
<institution>University of Tennessee - Knoxville</institution>
<lname>Budik</lname>
<fname>Daniel</fname>
<mname>Borisovich</mname>
</author>
</authors>
<disciplines><discipline>Electrical and Computer Engineering</discipline>
</disciplines><abstract>&lt;p&gt;Recurrent neural networks (RNNs) are widely acknowledged as an effective tool that can be employed by a wide range of applications that store and process temporal sequences. The ability of RNNs to capture complex, nonlinear system dynamics has served as a driving motivation for their study. RNNs have the potential to be effectively used in modeling, system identification and adaptive control applications, to name a few, where other techniques fall short. Most of the proposed RNN learning algorithms rely on the calculation of error gradients with respect to the network weights. What distinguishes recurrent neural networks from static, or feedforward networks, is the fact that the gradients are time-dependent or dynamic. This implies that the current error gradient does not only depend on the current input, output and targets, but rather on its possibly infinite past. How to effectively train RNNs remains a challenging and active research topic.&lt;/p&gt;
&lt;p&gt;This thesis introduces TRTRL, an efficient, low-complexity online learning algorithm for recurrent neural networks. The approach is based on the real-time recurrent learning (RTRL) algorithm, whereby the sensitivity set of each neuron is reduced to weights associated either with its input or output links. As a consequence, storage requirements are reduced from &lt;em&gt;O(N&lt;sup&gt;3&lt;/sup&gt;) &lt;/em&gt;to &lt;em&gt;O(N&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt; and the computational complexity is reduced from &lt;em&gt;O(N&lt;sup&gt;4&lt;/sup&gt;)&lt;/em&gt; to &lt;em&gt;O(N&lt;sup&gt;2&lt;/sup&gt;). &lt;/em&gt;Despite the radical reduction in resource requirements, it is shown through simulations results that the overall performance degradation of the truncated real-time recurrent learning (TRTRL) algorithm is minor. Moreover, the scheme lends itself to efficient hardware realization by virtue of the localized property that is inherent to the approach. The TRTRL algorithm is first implemented and evaluated using a multi-purpose CPU. Next, the framework is extended to a hardware implementation that scales to high network densities without compromising computation speed and overall performance.&lt;/p&gt;</abstract>
<coverpage-url>https://trace.tennessee.edu/utk_gradthes/1513</coverpage-url>
<fulltext-url>https://trace.tennessee.edu/cgi/viewcontent.cgi?article=2957&amp;amp;context=utk_gradthes&amp;amp;unstamped=1</fulltext-url>
<label>1513</label>
<document-type>thesis</document-type>
<type>article</type>
<articleid>2957</articleid>
<submission-date>2013-07-30T12:48:01-07:00</submission-date>
<publication-title>Masters Theses</publication-title>
<context-key>4361298</context-key>
<submission-path>utk_gradthes/1513</submission-path>
<fields>
<field name="advisor1" type="string">
<value>Itamar Elhanany</value>
</field>
<field name="advisor2" type="string">
<value>Donald Bouldin, Gregory Peterson</value>
</field>
<field name="degree_name" type="string">
<value>Master of Science</value>
</field>
<field name="department" type="string">
<value>Electrical Engineering</value>
</field>
<field name="embargo_date" type="date">
<value>2006-12-01T00:00:00-08:00</value>
</field>
<field name="publication_date" type="date">
<value>2006-12-01T00:00:00-08:00</value>
</field>
</fields>
</document>
</documents>