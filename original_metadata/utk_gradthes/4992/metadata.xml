<?xml version='1.0' encoding='iso-8859-1' ?>
<documents><document>
<title>Do you see what I mean? The role of visual speech information in lexical representations</title>
<publication-date>2017-12-16T00:00:00-08:00</publication-date>
<state>published</state>
<authors>
<author>
<email>rcannist@vols.utk.edu</email>
<institution>University of Tennessee</institution>
<lname>Cannistraci</lname>
<fname>Ryan</fname>
<mname>Andrew</mname>
</author>
</authors>
<keywords>
<keyword>Language learning</keyword>
<keyword>visual speech information</keyword>
<keyword>lexical representations</keyword>
<keyword>audiovisual communication</keyword>
</keywords>
<abstract>&lt;p&gt;Human speech is necessarily multimodal and audiovisual redundancies in speech may play a vital role in speech perception across the lifespan. The majority of previous studies have focused particularly on how language is learned from auditory input, but the way in which audiovisual speech information is perceived and comprehended remains less well understood. Here, I examine how audiovisual and visual-only speech information is represented for known words, and if intersensory processing efficiency ability predicts the strength of the lexical representation. To explore the relationship between intersensory processing ability (indexed by matching temporally synchronous auditory and visual stimulation) and the strength of lexical representations, adult subjects participated in an audiovisual word recognition task and the &lt;i&gt;Intersensory Processing Efficiency Protocol&lt;/i&gt; (IPEP). Participants were able to reliably identify a correct referent object across manipulations of modality (audiovisual vs visual-only) and pronunciation (correctly vs mispronounced). Correlational analyses did not reveal any relationship between processing efficiency and visual speech information in lexical representations. However, the results presented here suggest that adultsâ€™ lexical representations robustly include visual speech information and that visual speech information is sublexically processed during speech perception.&lt;/p&gt;</abstract>
<coverpage-url>https://trace.tennessee.edu/utk_gradthes/4992</coverpage-url>
<fulltext-url>https://trace.tennessee.edu/cgi/viewcontent.cgi?article=6399&amp;amp;context=utk_gradthes&amp;amp;unstamped=1</fulltext-url>
<label>4992</label>
<document-type>thesis</document-type>
<type>article</type>
<articleid>6399</articleid>
<submission-date>2018-11-08T14:23:49-08:00</submission-date>
<publication-title>Masters Theses</publication-title>
<context-key>13276614</context-key>
<submission-path>utk_gradthes/4992</submission-path>
<fields>
<field name="advisor1" type="string">
<value>Jessica Hay</value>
</field>
<field name="advisor2" type="string">
<value>Aaron Buss, Devin Casenhiser, Daniela Corbetta</value>
</field>
<field name="degree_name" type="string">
<value>Master of Arts</value>
</field>
<field name="department" type="string">
<value>Psychology</value>
</field>
<field name="publication_date" type="date">
<value>2017-12-16T00:00:00-08:00</value>
</field>
</fields>
</document>
</documents>